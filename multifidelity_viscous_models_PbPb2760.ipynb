{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=\"multifidelity_viscous_models_PbPb2760\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import GPy\n",
    "import os\n",
    "#import pickle\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from sklearn.decomposition import PCA\n",
    "#from numpy.linalg import inv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as gpr\n",
    "from sklearn.gaussian_process import kernels as krnl\n",
    "import scipy.stats as st\n",
    "from scipy import optimize\n",
    "\n",
    "#import emcee\n",
    "#import ptemcee\n",
    "#import h5py\n",
    "#from scipy.linalg import lapack\n",
    "from multiprocessing import Pool\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup working folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures and data files\n",
    "PROJECT_ROOT_DIR = \"Results_ldr\"\n",
    "FIGURE_ID = \"Results_ldr/FigureFiles\"\n",
    "DATA_ID = \"DataFiles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PROJECT_ROOT_DIR):\n",
    "    os.mkdir(PROJECT_ROOT_DIR)\n",
    "\n",
    "if not os.path.exists(FIGURE_ID):\n",
    "    os.makedirs(FIGURE_ID)\n",
    "\n",
    "if not os.path.exists(DATA_ID):\n",
    "    os.makedirs(DATA_ID)\n",
    "\n",
    "def image_path(fig_id):\n",
    "    return os.path.join(FIGURE_ID, fig_id)\n",
    "\n",
    "def data_path(dat_id):\n",
    "    return os.path.join(DATA_ID, dat_id)\n",
    "\n",
    "def save_fig(fig_id):\n",
    "    plt.savefig(image_path(fig_id) + \".png\", format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the simulation data from the four viscosity correction models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            0 : 'Grad',\n",
    "            1 : 'Chapman-Enskog R.T.A',\n",
    "            2 : 'Pratt-McNelis',\n",
    "            3 : 'Pratt-Bernhard'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds for parametrs in the emulator are same as prior ranges so\n",
    "prior_df = pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_prior\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design points\n",
    "design = pd.read_csv(filepath_or_buffer=\"DataFiles/PbPb2760_design\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation outputs at the design points\n",
    "simulation_df = []\n",
    "simulation_sd_df = []\n",
    "for idf in range(0,4):\n",
    "    simulation_df.append(pd.read_csv(filepath_or_buffer=f\"DataFiles/PbPb2760_simulation_{idf}\"))\n",
    "    #simulation_sd_df.append(pd.read_csv(filepath_or_buffer=f\"DataFiles/PbPb2760_simulation_error_{idf}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clms=simulation_df[1].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize all the other models using mean and variance of observable in the Grad model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data with respect to lower fidelity\n",
    "s_l = StandardScaler()\n",
    "x = simulation_df[0].values\n",
    "s_l.fit(x)\n",
    "for idf in range(0,4):\n",
    "    x_tmp = simulation_df[idf].values\n",
    "    simulation_df[idf]= pd.DataFrame(s_l.transform(x_tmp),columns=df_clms)\n",
    "\n",
    "#diff = np.array(prior_df.loc['max'].values - prior_df.loc['min'].values ).reshape(1,-1)\n",
    "#diff_mat = np.repeat(diff,X.shape[0],axis=0)\n",
    "#print(f'Shape of diff matt {diff_mat.shape}')\n",
    "#X= np.divide(X,diff_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = design.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape : (485, 17)\n",
      "Y.shape : (485, 110)\n",
      "X.shape : (485, 17)\n",
      "Y.shape : (485, 110)\n",
      "X.shape : (485, 17)\n",
      "Y.shape : (485, 110)\n",
      "X.shape : (485, 17)\n",
      "Y.shape : (485, 110)\n"
     ]
    }
   ],
   "source": [
    "for idf in range(0,4):\n",
    "    Y = simulation_df[idf].values\n",
    "    print( \"X.shape : \"+ str(X.shape) )\n",
    "    print( \"Y.shape : \"+ str(Y.shape) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels for plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameter names in Latex compatble form\n",
    "model_param_dsgn = ['$N$[$2.76$TeV]',\n",
    " '$p$',\n",
    " '$\\\\sigma_k$',\n",
    " '$w$ [fm]',\n",
    " '$d_{\\\\mathrm{min}}$ [fm]',\n",
    " '$\\\\tau_R$ [fm/$c$]',\n",
    " '$\\\\alpha$',\n",
    " '$T_{\\\\eta,\\\\mathrm{kink}}$ [GeV]',\n",
    " '$a_{\\\\eta,\\\\mathrm{low}}$ [GeV${}^{-1}$]',\n",
    " '$a_{\\\\eta,\\\\mathrm{high}}$ [GeV${}^{-1}$]',\n",
    " '$(\\\\eta/s)_{\\\\mathrm{kink}}$',\n",
    " '$(\\\\zeta/s)_{\\\\max}$',\n",
    " '$T_{\\\\zeta,c}$ [GeV]',\n",
    " '$w_{\\\\zeta}$ [GeV]',\n",
    " '$\\\\lambda_{\\\\zeta}$',\n",
    " '$b_{\\\\pi}$',\n",
    " '$T_{\\\\mathrm{sw}}$ [GeV]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "observables_latex_2 = ['$\\\\frac{dN_{ch}}{d\\\\eta}$',\n",
    " '$\\\\frac{dE_T}{d\\\\eta}$',\n",
    " '$\\\\frac{dN_{\\\\pi}}{dy}$',\n",
    " '$\\\\frac{dN_{K}}{dy}$',\n",
    " '$\\\\frac{dN_{P}}{dy}$',\n",
    " '$\\\\langle pT_{\\\\pi} \\\\rangle$',\n",
    " '$\\\\langle pT_{K} \\\\rangle$',\n",
    " '$\\\\langle pT_{P} \\\\rangle$',\n",
    " '$\\\\frac{\\\\delta p_T}{\\\\langle p_T \\\\rangle}$',\n",
    " '$v_2${2}',\n",
    " '$v_3${2}',\n",
    " '$v_4${2}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "observables_latex = ['$dN_{ch} / d\\\\eta$',\n",
    " '$dE_T / d\\\\eta$',\n",
    " '${dN_{\\\\pi}} / {dy}$',\n",
    " '${dN_{K}} / {dy}$',\n",
    " '${dN_{P}} / {dy}$',\n",
    " '$\\\\langle p_{T, \\\\pi} \\\\rangle$',\n",
    " '$\\\\langle p_{T, K} \\\\rangle$',\n",
    " '$\\\\langle p_{T, P} \\\\rangle$',\n",
    " '${\\\\delta p_T} / {\\\\langle p_T \\\\rangle}$',\n",
    " '$v_2${2}',\n",
    " '$v_3${2}',\n",
    " '$v_4${2}']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observables considered in this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pT_fluct[10 15]', 'pT_fluct[15 20]', 'pT_fluct[20 25]',\n",
       "       'pT_fluct[25 30]', 'pT_fluct[30 35]', 'pT_fluct[35 40]',\n",
       "       'pT_fluct[40 45]', 'pT_fluct[45 50]', 'pT_fluct[50 55]',\n",
       "       'pT_fluct[55 60]', 'v22[0 5]', 'v22[ 5 10]', 'v22[10 20]', 'v22[20 30]',\n",
       "       'v22[30 40]', 'v22[40 50]', 'v22[50 60]', 'v22[60 70]', 'v32[0 5]',\n",
       "       'v32[ 5 10]'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_df[1].keys()[80:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "observables_choosen = ['dNch_deta[0 5]',\n",
    " 'dNch_deta[60 70]',\n",
    " 'dN_dy_pion[0 5]',\n",
    " 'dN_dy_pion[60 70]',\n",
    "# 'dN_dy_kaon[0 5]',\n",
    "# 'dN_dy_kaon[60 70]',\n",
    "# 'dN_dy_proton[0 5]',\n",
    "# 'dN_dy_proton[60 70]',\n",
    " 'mean_pT_pion[0 5]',\n",
    " 'mean_pT_pion[60 70]',\n",
    "# 'mean_pT_kaon[0 5]',\n",
    "# 'mean_pT_kaon[60 70]',               \n",
    "# 'mean_pT_proton[0 5]',\n",
    "# 'mean_pT_proton[60 70]',\n",
    " 'pT_fluct[0 5]',\n",
    " 'pT_fluct[55 60]',\n",
    " 'v22[0 5]',\n",
    "  'v22[60 70]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear multifidelity modeling using Emukit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPy\n",
    "import emukit.multi_fidelity\n",
    "from emukit.model_wrappers.gpy_model_wrappers import GPyMultiOutputWrapper\n",
    "from emukit.multi_fidelity.models import GPyLinearMultiFidelityModel\n",
    "## Convert lists of arrays to ndarrays augmented with fidelity indicators\n",
    "from sklearn.model_selection import train_test_split\n",
    "from emukit.multi_fidelity.convert_lists_to_array import convert_x_list_to_array, convert_xy_lists_to_arrays\n",
    "from sklearn.model_selection import KFold\n",
    "#from emukit.multi_fidelity.models.non_linear_multi_fidelity_model import make_non_linear_kernels, NonLinearMultiFidelityModel\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_train_and_validation(x_train_l, x_train_h, x_test_h, y_train_l, y_train_h, y_test_h, obs_name):\n",
    "    \"\"\" Train two types of emulators. And perform validation on a given set\n",
    "            1. Linear multifidelity GPs\n",
    "            2. Standard GP\n",
    "        =============================\n",
    "        Return:\n",
    "            3 x 2 ndarray. and rho value of linear multifidelity model\n",
    "            First row has MSE\n",
    "            Second row has R2 scores\n",
    "            Third row has the White noise variance\n",
    "        \"\"\"\n",
    "    print('########################')\n",
    "    print(obs_name)\n",
    "    X_train, Y_train = convert_xy_lists_to_arrays([x_train_l, x_train_h], [y_train_l, y_train_h])\n",
    "    n_opt = 5\n",
    "    ## Construct a linear multi-fidelity model\n",
    "\n",
    "    #kernels = [GPy.kern.RBF(17, ARD=True), GPy.kern.RBF(17, ARD=True)]\n",
    "    kernels = [GPy.kern.Matern52(17, ARD=True), GPy.kern.Matern52(17, ARD=True)]\n",
    "    lin_mf_kernel = emukit.multi_fidelity.kernels.LinearMultiFidelityKernel(kernels)\n",
    "    gpy_lin_mf_model = GPyLinearMultiFidelityModel(X_train, Y_train, lin_mf_kernel, n_fidelities=2)\n",
    "    #gpy_lin_mf_model.mixed_noise.Gaussian_noise.fix(0)\n",
    "    gpy_lin_mf_model.mixed_noise.Gaussian_noise_1.fix(0)\n",
    "\n",
    "\n",
    "    ## Wrap the model using the given 'GPyMultiOutputWrapper'\n",
    "    lin_mf_model = GPyMultiOutputWrapper(gpy_lin_mf_model, 2, n_optimization_restarts=n_opt)\n",
    "\n",
    "    ## Fit the model\n",
    "  \n",
    "    lin_mf_model.optimize()\n",
    "    \n",
    "    ## Create standard GP model using only high-fidelity data\n",
    "\n",
    "    kernel = GPy.kern.RBF(input_dim=17, ARD=True)\n",
    "    high_gp_model = GPy.models.GPRegression(x_train_h,y_train_h, kernel)\n",
    "    #high_gp_model.Gaussian_noise.fix(0)\n",
    "\n",
    "    ## Fit the GP model\n",
    "    \n",
    "    high_gp_model.optimize_restarts(n_opt, verbose=True, parallel=True, num_processes=5)\n",
    "    #print(high_gp_model)\n",
    "    ## Compute mean predictions and associated variance\n",
    "\n",
    "    #hf_mean_high_gp_model, hf_var_high_gp_model  = high_gp_model.predict(x_plot)\n",
    "    #hf_std_hf_gp_model = np.sqrt(hf_var_high_gp_model)\n",
    "    x_temp = convert_x_list_to_array([x_test_h,x_test_h])\n",
    "    x_test_h_idf_index = x_temp[x_test_h.shape[0]:,:]\n",
    "    x_test_l_idf_index = x_temp[0:x_test_h.shape[0],:]\n",
    "    \n",
    "    hf_mean_lin_mf_model, hf_var_lin_mf_model = lin_mf_model.predict(x_test_h_idf_index)\n",
    "    lf_mean_lin_mf_model, lf_var_lin_mf_model = lin_mf_model.predict(x_test_l_idf_index)\n",
    "\n",
    "\n",
    "    print(hf_mean_lin_mf_model.shape)\n",
    "    r2_1 = r2_score(y_test_h,hf_mean_lin_mf_model )\n",
    "    mse_1 = mean_squared_error(y_test_h,hf_mean_lin_mf_model )\n",
    "    print(f'r2 score for multifidelity linear {r2_score(y_test_h,hf_mean_lin_mf_model )}')\n",
    "    print(f'mse for multifidelity linear {mean_squared_error(y_test_h,hf_mean_lin_mf_model )}')\n",
    "\n",
    "    hf_mean_high_gp_model, hf_var_high_gp_model  = high_gp_model.predict(x_test_h)\n",
    "    print(hf_mean_high_gp_model.shape)\n",
    "    r2_3 = r2_score(y_test_h,hf_mean_high_gp_model)\n",
    "    mse_3 = mean_squared_error(y_test_h,hf_mean_high_gp_model )\n",
    "    print(f'r2 score for standard GP {r2_score(y_test_h,hf_mean_high_gp_model)}')\n",
    "    print(f'mse for standard GP {mean_squared_error(y_test_h,hf_mean_high_gp_model )}')\n",
    "    \n",
    "    #Plots\n",
    "    \n",
    "    #corner plots for high fidelity input parameters\n",
    "    fraction_points=10 #int(np.ceil(0.1* x_train_h.shape[0]))\n",
    "    print(f'Fraction of points {fraction_points}')\n",
    "    \n",
    "    #plot input parameters for few poor validation points\n",
    "    diff_test_lin=np.array(np.absolute(hf_mean_lin_mf_model-y_test_h))\n",
    "    sort_index=np.argsort(diff_test_lin.flatten())\n",
    "    worst= sort_index[-fraction_points:]\n",
    "    print(f'Worst {fraction_points} of validations')\n",
    "    print('######################')\n",
    "    print(diff_test_lin[worst])\n",
    "    #print(worst.shape)\n",
    "    matplotlib.rcParams.update({'font.size': 20})\n",
    "    fig1, axs= plt.subplots(17,17, figsize=(70,70))\n",
    "    fig1.suptitle(f'{obs_name}_Worst validation for batch size {x_train_h.shape[0]}')\n",
    "    for row in range(0,17):\n",
    "        for clmn in range(0,17):\n",
    "            if row < clmn:\n",
    "                ax = axs[row,clmn]\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            if row == clmn:\n",
    "                ax = axs[row,clmn]\n",
    "                ax.hist(x_train_h[:,clmn],color='blue',density=True, alpha=0.5)\n",
    "                ax.hist(x_test_h[worst,clmn], color='red', density=True, alpha=0.5)\n",
    "            else:\n",
    "                ax= axs[row,clmn]\n",
    "                if row == 16:\n",
    "                    ax.set_xlabel(model_param_dsgn[clmn], fontsize =40)\n",
    "                if clmn == 0:\n",
    "                    ax.set_ylabel(model_param_dsgn[row], fontsize =40)\n",
    "                ax.scatter(x_train_h[:,clmn],x_train_h[:,row], cmap='Greens', label = 'training')\n",
    "                ax.scatter(x_test_h[:,clmn],x_test_h[:,row], cmap='Reds', c=diff_test_lin, label='worst validation')\n",
    "    plt.tight_layout()\n",
    "    save_fig(f'{obs_name}_{name}_{x_train_h.shape[0]}_worst_validation')\n",
    "    #Plot the discrepency function contribution and the low fidelity contribution for validations seprately\n",
    "    discrepency_prediction_lin_mf = np.divide(hf_mean_lin_mf_model - lf_mean_lin_mf_model,lf_mean_lin_mf_model)\n",
    "    #discrepency_prediction_nonlin_mf = np.divide(hf_mean_nonlin_mf_model - lf_mean_nonlin_mf_model,lf_mean_nonlin_mf_model)\n",
    "    print('Average discrepency ratio for linear mf model')\n",
    "    print('#######################################')\n",
    "    print(np.mean(np.absolute(discrepency_prediction_lin_mf)))\n",
    "#     print('#######################################')    \n",
    "#     print('Average discrepency ratio for non linear mf model')\n",
    "#     print(np.mean(np.absolute(discrepency_prediction_nonlin_mf)))\n",
    "    matplotlib.rcParams.update({'font.size': 20})\n",
    "    fig2, axs= plt.subplots(17,17, figsize=(70,70))\n",
    "    fig2.suptitle(f'{obs_name}_biggest discrpency multifidelity GP for batch size {x_train_h.shape[0]}')\n",
    "    #plot input parameters for few poor validation points\n",
    "    disc_test_lin=np.array(np.absolute(discrepency_prediction_lin_mf))\n",
    "    #print(diff_test_lin)\n",
    "    sort_index=np.argsort(disc_test_lin.flatten())\n",
    "    #print(sort_index)\n",
    "    worst_disc_lin= sort_index[-fraction_points:]\n",
    "    for row in range(0,17):\n",
    "        for clmn in range(0,17):\n",
    "            if row < clmn:\n",
    "                ax = axs[row,clmn]\n",
    "                ax.axis('off')\n",
    "                continue\n",
    "            if row == clmn:\n",
    "                ax = axs[row,clmn]\n",
    "                ax.axis('off')\n",
    "                #ax.hist(x_train_h[:,clmn],color='green',density=True)\n",
    "                #ax.hist(x_test_h[worst,clmn], color='red', density=True)\n",
    "            else:\n",
    "                ax= axs[row,clmn]\n",
    "                if row == 16:\n",
    "                    ax.set_xlabel(model_param_dsgn[clmn], fontsize =40)\n",
    "                if clmn == 0:\n",
    "                    ax.set_ylabel(model_param_dsgn[row], fontsize =40)\n",
    "                ax.scatter(x_test_h[worst_disc_lin,clmn],x_test_h[worst_disc_lin,row], cmap='Reds',c= discrepency_prediction_lin_mf.flatten()[worst_disc_lin], label = 'linear')\n",
    "                #ax.scatter(x_test_h[worst_disc_lin,clmn],x_test_h[worst_disc_lin,row], cmap='Reds',c= discrepency_prediction_lin_mf.flatten()[worst_disc_lin], label = 'linear')\n",
    "    plt.tight_layout()\n",
    "    save_fig(f'{obs_name}_{name}_{x_train_h.shape[0]}_discrepency')\n",
    "    \n",
    "    #Plot QQ plots? // uncertainty prediction magnitudes? \n",
    "   # print(f'shape of test linear mean {hf_mean_lin_mf_model.shape}')\n",
    "   # print(f'shape of test linear variance {hf_var_lin_mf_model.shape}')\n",
    "   # print(f'shape of test nonlinear mean {hf_mean_nonlin_mf_model.shape}')\n",
    "   # print(f'shape of test nonlinear variance {hf_var_nonlin_mf_model.shape}')\n",
    "   # print(f'shape of test standard GP mean {hf_mean_high_gp_model.shape}')\n",
    "   # print(f'shape of test standard GP variance {hf_var_high_gp_model.shape}')\n",
    "    lin_mf_normalize = np.divide(np.array(hf_mean_lin_mf_model-y_test_h).reshape(-1,1),np.sqrt(hf_var_lin_mf_model.reshape(-1,1)))\n",
    "   # nonlin_mf_normalize = np.divide(np.array(hf_mean_nonlin_mf_model-y_test_h).reshape(-1,1),np.sqrt(hf_var_nonlin_mf_model.reshape(-1,1))) \n",
    "    standard_gp_normalize = np.divide(np.array(hf_mean_high_gp_model-y_test_h).reshape(-1,1),np.sqrt(hf_var_high_gp_model.reshape(-1,1))) \n",
    "\n",
    "    fig3, ax = plt.subplots(figsize=(10,10))\n",
    "    percs = np.linspace(5,95,19)\n",
    "    qn_a = np.percentile(lin_mf_normalize, percs)\n",
    "  #  qn_b = np.percentile(nonlin_mf_normalize, percs)\n",
    "    qn_c = np.percentile(standard_gp_normalize, percs)\n",
    "    qn_th = st.norm.ppf(q=0.01*np.linspace(5,95,19))\n",
    "    ax.plot(qn_th,qn_a, ls=\"\", marker=\"o\", label = 'Linear')\n",
    "  #  ax.plot(qn_th,qn_b, ls=\"\", marker=\"o\", label = 'Nonlinear')\n",
    "    ax.plot(qn_th,qn_c, ls=\"\", marker=\"o\", label = 'Standard')\n",
    "    x = np.linspace(np.min((qn_a.min(),qn_c.min())), np.max((qn_a.max(),qn_c.max())))\n",
    "    ax.plot(x,x, color=\"k\", ls=\"--\")\n",
    "    ax.legend()\n",
    "    ax.set_xlabel('Theorotical quantiles')\n",
    "    ax.set_ylabel(f'{obs_name}_Emulation quantiles')\n",
    "    plt.tight_layout()\n",
    "    save_fig(f'{obs_name}_{name}_{x_train_h.shape[0]}_QQ_plot')\n",
    "    \n",
    "    r2s= [r2_1, r2_3]\n",
    "    mses=[mse_1, mse_3]\n",
    "    Wn = [lin_mf_model.gpy_model.param_array[-2], high_gp_model.param_array[-1]]\n",
    "    rho = lin_mf_model.gpy_model.param_array[-3]\n",
    "    return np.array([mses, r2s, Wn]), rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "dNch_deta[0 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/dananjayaliyanage/miniconda3/envs/parton_loss/lib/python3.6/site-packages/paramz/transformations.py:111: RuntimeWarning:divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = -39.61621197862621\n",
      "Optimization restart 2/5, f = -38.85965567645863\n",
      "Optimization restart 3/5, f = -39.40649184002132\n",
      "Optimization restart 4/5, f = -39.218473523219984\n",
      "Optimization restart 5/5, f = -40.193526626088214\n",
      "Optimization restart 1/5, f = 16.272004381765207\n",
      "Optimization restart 2/5, f = 50.95435476800239\n",
      "Optimization restart 3/5, f = 16.271994985890498\n",
      "Optimization restart 4/5, f = 55.31045222934547\n",
      "Optimization restart 5/5, f = 16.271999013749646\n",
      "(97, 1)\n",
      "r2 score for multifidelity linear 0.9608275894226321\n",
      "mse for multifidelity linear 0.028281994473427233\n",
      "(97, 1)\n",
      "r2 score for standard GP 0.8644554412230872\n",
      "mse for standard GP 0.0978614899039834\n",
      "Fraction of points 10\n",
      "Worst 10 of validations\n",
      "######################\n",
      "[[0.25992273]\n",
      " [0.27694627]\n",
      " [0.29871173]\n",
      " [0.32882033]\n",
      " [0.33822959]\n",
      " [0.35977354]\n",
      " [0.44856092]\n",
      " [0.52483031]\n",
      " [0.59074491]\n",
      " [0.62695502]]\n",
      "Average discrepency ratio for linear mf model\n",
      "#######################################\n",
      "2.385523460027754\n",
      "########################\n",
      "dNch_deta[0 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/dananjayaliyanage/miniconda3/envs/parton_loss/lib/python3.6/site-packages/paramz/transformations.py:111: RuntimeWarning:divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization restart 1/5, f = -485.07051994074357\n",
      "Optimization restart 2/5, f = 637.797250832107\n",
      "Optimization restart 3/5, f = 638.6678498810529\n",
      "Optimization restart 4/5, f = 261.15443907792337\n",
      "Optimization restart 5/5, f = -486.3731450234674\n",
      "Optimization restart 1/5, f = 69.6284072337483\n",
      "Optimization restart 2/5, f = 69.55995798842343\n",
      "Optimization restart 3/5, f = 69.56261371368828\n",
      "Optimization restart 4/5, f = 69.56189907586517\n",
      "Optimization restart 5/5, f = 69.6023975201976\n",
      "(97, 1)\n",
      "r2 score for multifidelity linear 0.9799469755156973\n",
      "mse for multifidelity linear 0.014478034904704513\n",
      "(97, 1)\n",
      "r2 score for standard GP 0.9812765675970834\n",
      "mse for standard GP 0.013518085916540953\n",
      "Fraction of points 10\n",
      "Worst 10 of validations\n",
      "######################\n",
      "[[0.1923161 ]\n",
      " [0.19425618]\n",
      " [0.19873719]\n",
      " [0.23149728]\n",
      " [0.25269447]\n",
      " [0.28782254]\n",
      " [0.28818825]\n",
      " [0.29791694]\n",
      " [0.32736981]\n",
      " [0.45427265]]\n",
      "Average discrepency ratio for linear mf model\n",
      "#######################################\n",
      "2.5092920707297997\n",
      "########################\n",
      "dNch_deta[60 70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/dananjayaliyanage/miniconda3/envs/parton_loss/lib/python3.6/site-packages/paramz/transformations.py:111: RuntimeWarning:divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "n_batch = 10\n",
    "\n",
    "r2_ar_obs = []\n",
    "mse_ar_obs = []\n",
    "batch_ar_obs = []\n",
    "wn_ar_obs = []\n",
    "rho_ar_obs = []\n",
    "\n",
    "for selected_observable in observables_choosen:\n",
    "\n",
    "    Y_l=simulation_df[0][selected_observable].values.reshape(-1,1)\n",
    "    Y_h=simulation_df[3][selected_observable].values.reshape(-1,1)\n",
    "    \n",
    "    r2_ar_crs = []\n",
    "    mse_ar_crs = []\n",
    "    batch_ar_crs = []\n",
    "    wn_ar_crs = []\n",
    "    rho_ar_crs = []\n",
    "    for split_i,[train_index, test_index] in enumerate(kf.split(X)):\n",
    "        if split_i>0:\n",
    "            break\n",
    "        r2_ar=[]\n",
    "        mse_ar=[]\n",
    "        batch_ar=[]\n",
    "        wn_ar=[]\n",
    "        rho_ar=[]\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        x_train_h, x_test_h, y_train_h, y_test_h = X[train_index,:], X[test_index], Y_h[train_index,:], Y_h[test_index,:]\n",
    "        x_train_l, x_test_l, y_train_l, y_test_l = X[train_index,:], X[test_index], Y_l[train_index,:], Y_l[test_index,:]\n",
    "    \n",
    "        for i in range(0,n_batch):\n",
    "    \n",
    "            l=0\n",
    "            h=(train_index.shape[0]//n_batch)*(i+1)\n",
    "            #if i == n_batch-1:\n",
    "            if i == 1:\n",
    "                h=train_index.shape[0]\n",
    "            if i >1:\n",
    "                break\n",
    "    ## Construct a linear multi-fidelity model\n",
    "            r2_mse_wn,rho = run_train_and_validation(x_train_l, x_train_h[l:h,:],x_test_h, y_train_l, y_train_h[l:h,:], y_test_h, selected_observable)\n",
    "            r2_ar.append(r2_mse_wn[0,:])\n",
    "            mse_ar.append(r2_mse_wn[1,:])\n",
    "            wn_ar.append(r2_mse_wn[2,:])\n",
    "            batch_ar.append(h)\n",
    "            rho_ar.append(rho)\n",
    "        r2_ar = np.array(r2_ar)\n",
    "        mse_ar = np.array(mse_ar)\n",
    "        batch_ar = np.array(batch_ar)\n",
    "        wn_ar = np.array(wn_ar)\n",
    "        rho_ar = np.array(rho_ar)\n",
    "    \n",
    "        r2_ar_crs.append(r2_ar)\n",
    "        mse_ar_crs.append(mse_ar)\n",
    "        batch_ar_crs.append(batch_ar)\n",
    "        wn_ar_crs.append(wn_ar)\n",
    "        rho_ar_crs.append(rho_ar)\n",
    "    r2_ar_crs = np.array(r2_ar_crs)\n",
    "    mse_ar_crs = np.array(mse_ar_crs)\n",
    "    batch_ar_crs = np.array(batch_ar_crs)\n",
    "    wn_ar_crs = np.array(wn_ar_crs)\n",
    "    rho_ar_crs = np.array(rho_ar_crs)\n",
    "    \n",
    "    r2_ar_obs.append(r2_ar_crs)\n",
    "    mse_ar_obs.append(mse_ar_crs)\n",
    "    batch_ar_obs.append(batch_ar_crs)\n",
    "    wn_ar_obs.append(wn_ar_crs)\n",
    "    rho_ar_obs.append(rho_ar_crs)\n",
    "    \n",
    "r2_ar_obs = np.array(r2_ar_obs)\n",
    "mse_ar_obs = np.array(mse_ar_obs)\n",
    "batch_ar_obs = np.array(batch_ar_obs)\n",
    "wn_ar_obs = np.array(wn_ar_obs)\n",
    "rho_ar_obs = np.array(rho_ar_obs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax =plt.subplots(nrows=1, ncols=2, figsize=(20,10))\n",
    "# ax1, ax2 = ax\n",
    "# ax1.plot(batch_ar,r2_ar[:,0], c ='r', label='multifidelity linear')\n",
    "# ax1.plot(batch_ar,r2_ar[:,1], c ='y', label='multifidelity nonlinear')\n",
    "# ax1.plot(batch_ar,r2_ar[:,2], c ='c', label='Standard GP')\n",
    "#     #l,h=ax.get_ylim()\n",
    "#     #line_1d = np.linspace(l,h,100)\n",
    "#     #ax.plot(line_1d,line_1d)\n",
    "# ax1.set_xlabel('Number of high fidelity points used in training')\n",
    "# ax1.set_ylabel('R2 score')\n",
    "# ax1.legend()\n",
    "    \n",
    "# ax2.plot(batch_ar,mse_ar[:,0], c ='r', label='multifidelity linear')\n",
    "# ax2.plot(batch_ar,mse_ar[:,1], c ='y', label='multifidelity nonlinear')\n",
    "# ax2.plot(batch_ar,mse_ar[:,2], c ='c', label='Standard GP')\n",
    "# ax2.set_xlabel('Number of high fidelity points used in training')\n",
    "# ax2.set_ylabel('MSE')\n",
    "# ax2.legend()\n",
    "# save_fig(f'{val_i}_batchsize_{n_batch}_ARD_PTB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
